{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02dba483",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama3_transformer_block import *\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a65376c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_HEAD = 16\n",
    "NUM_KV_HEAD = 8\n",
    "NUM_LAYER = 4\n",
    "EMBED_DIM = 512\n",
    "HEAD_DIM = EMBED_DIM // NUM_HEAD\n",
    "ROPE_BASE = 10000\n",
    "MLP_SCALE = 3.5\n",
    "EPS_NORM = 1e-5\n",
    "DROPOUT = math.sin(math.sqrt(math.e * math.pi))\n",
    "MAX_SEQUENCE = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6702fb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, INPUT_DIM):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.INPUT_DIM = INPUT_DIM\n",
    "        self.TOKEN_EMBEDDING = nn.Embedding(INPUT_DIM, EMBED_DIM)\n",
    "\n",
    "        self.SELF_ATTENTION = CausalSelfAttention(\n",
    "            embed_dim=EMBED_DIM,\n",
    "            num_heads=NUM_HEAD,\n",
    "            num_kv_heads=NUM_KV_HEAD,\n",
    "            head_dim=HEAD_DIM,\n",
    "            q_proj=nn.Linear(EMBED_DIM, EMBED_DIM, bias=False),\n",
    "            k_proj=nn.Linear(EMBED_DIM, NUM_KV_HEAD * HEAD_DIM, bias=False),\n",
    "            v_proj=nn.Linear(EMBED_DIM, NUM_KV_HEAD * HEAD_DIM, bias=False),\n",
    "            output_proj=nn.Linear(EMBED_DIM, EMBED_DIM, bias=False),\n",
    "            pos_embeddings=RotaryPositionalEmbedding(\n",
    "                dim=HEAD_DIM,\n",
    "                max_seq_len=MAX_SEQUENCE,\n",
    "                base=ROPE_BASE,\n",
    "            ),\n",
    "            max_seq_len=MAX_SEQUENCE,\n",
    "            attn_dropout=DROPOUT,\n",
    "        )\n",
    "        self.MLP = FeedForward(\n",
    "            gate_proj=nn.Linear(EMBED_DIM, int(EMBED_DIM * MLP_SCALE), bias=False),\n",
    "            down_proj=nn.Linear(int(EMBED_DIM * MLP_SCALE), EMBED_DIM, bias=False),\n",
    "            up_proj=nn.Linear(EMBED_DIM, int(EMBED_DIM * MLP_SCALE), bias=False),\n",
    "        )\n",
    "\n",
    "        self.ENCODER_LAYER = TransformerEncoderLayer(\n",
    "            attn=self.SELF_ATTENTION,\n",
    "            mlp=copy.deepcopy(self.MLP),\n",
    "            sa_norm=RMSNorm(dim=EMBED_DIM, eps=EPS_NORM),\n",
    "            mlp_norm=RMSNorm(dim=EMBED_DIM, eps=EPS_NORM),\n",
    "        )\n",
    "\n",
    "        self.encoder = TransformerEncoder(\n",
    "            tok_embedding=self.TOKEN_EMBEDDING,\n",
    "            layer=self.ENCODER_LAYER,\n",
    "            num_layers=NUM_LAYER,\n",
    "            max_seq_len=MAX_SEQUENCE,\n",
    "            num_heads=NUM_HEAD,\n",
    "            head_dim=HEAD_DIM,\n",
    "            norm=RMSNorm(EMBED_DIM, eps=EPS_NORM),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.encoder(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4606cf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, OUTPUT_DIM):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.OUTPUT_DIM = OUTPUT_DIM\n",
    "\n",
    "        self.TOKEN_EMBEDDING = nn.Embedding(OUTPUT_DIM, EMBED_DIM)\n",
    "        \n",
    "        self.MLP = FeedForward(\n",
    "            gate_proj=nn.Linear(EMBED_DIM, int(EMBED_DIM * MLP_SCALE), bias=False),\n",
    "            down_proj=nn.Linear(int(EMBED_DIM * MLP_SCALE), EMBED_DIM, bias=False),\n",
    "            up_proj=nn.Linear(EMBED_DIM, int(EMBED_DIM * MLP_SCALE), bias=False),\n",
    "        )\n",
    "        self.ROPE = RotaryPositionalEmbedding(\n",
    "            dim=HEAD_DIM,\n",
    "            max_seq_len=MAX_SEQUENCE,\n",
    "            base=ROPE_BASE,\n",
    "        )\n",
    "        self.SELF_ATTENTION_1 = CausalSelfAttention(\n",
    "            embed_dim=EMBED_DIM,\n",
    "            num_heads=NUM_HEAD,\n",
    "            num_kv_heads=NUM_KV_HEAD,\n",
    "            head_dim=HEAD_DIM,\n",
    "            q_proj=nn.Linear(EMBED_DIM, EMBED_DIM, bias=False),\n",
    "            k_proj=nn.Linear(EMBED_DIM, NUM_KV_HEAD * HEAD_DIM, bias=False),\n",
    "            v_proj=nn.Linear(EMBED_DIM, NUM_KV_HEAD * HEAD_DIM, bias=False),\n",
    "            output_proj=nn.Linear(EMBED_DIM, EMBED_DIM, bias=False),\n",
    "            pos_embeddings=self.ROPE,\n",
    "            max_seq_len=MAX_SEQUENCE,\n",
    "            attn_dropout=DROPOUT,\n",
    "        )\n",
    "        self.SELF_ATTENTION_2 = CausalSelfAttention(\n",
    "            embed_dim=EMBED_DIM,\n",
    "            num_heads=NUM_HEAD,\n",
    "            num_kv_heads=NUM_KV_HEAD,\n",
    "            head_dim=HEAD_DIM,\n",
    "            q_proj=nn.Linear(EMBED_DIM, EMBED_DIM, bias=False),\n",
    "            k_proj=nn.Linear(EMBED_DIM, NUM_KV_HEAD * HEAD_DIM, bias=False),\n",
    "            v_proj=nn.Linear(EMBED_DIM, NUM_KV_HEAD * HEAD_DIM, bias=False),\n",
    "            output_proj=nn.Linear(EMBED_DIM, EMBED_DIM, bias=False),\n",
    "            pos_embeddings=self.ROPE,\n",
    "            max_seq_len=MAX_SEQUENCE,\n",
    "            attn_dropout=DROPOUT,\n",
    "        )\n",
    "        self.DECODER_LAYER = TransformerDecoderLayer(\n",
    "            attn1=self.SELF_ATTENTION_1,\n",
    "            attn2=self.SELF_ATTENTION_2,\n",
    "            mlp=copy.deepcopy(self.MLP),\n",
    "            sa_norm_x1=RMSNorm(dim=EMBED_DIM, eps=EPS_NORM),\n",
    "            sa_norm_x2=RMSNorm(dim=EMBED_DIM, eps=EPS_NORM),\n",
    "            mlp_norm=RMSNorm(dim=EMBED_DIM, eps=EPS_NORM),\n",
    "        )\n",
    "        self.OUT_PROJECTION = nn.Linear(EMBED_DIM, OUTPUT_DIM, bias=False)\n",
    "        self.decoder = TransformerDecoder(\n",
    "            tok_embedding=self.TOKEN_EMBEDDING,\n",
    "            layer=self.DECODER_LAYER,\n",
    "            num_layers=NUM_LAYER,\n",
    "            max_seq_len=MAX_SEQUENCE,\n",
    "            num_heads=NUM_HEAD,\n",
    "            head_dim=HEAD_DIM,\n",
    "            norm=RMSNorm(EMBED_DIM, eps=EPS_NORM),\n",
    "            output=self.OUT_PROJECTION,\n",
    "        )\n",
    "\n",
    "    def forward(self, x, encoder_out):\n",
    "        out = self.decoder(x, encoder_out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d6a34c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationModel(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(TranslationModel, self).__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x, y): # x is input sequence, y is target sequence\n",
    "        encoder_out = self.encoder(x)\n",
    "        decoder_out = self.decoder(y, encoder_out)\n",
    "        return decoder_out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc34323e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vi_data_link = \"viet-lao-dataset/data-lao-viet/data.vi\"\n",
    "lao_data_link = \"viet-lao-dataset/data-lao-viet/data.lo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ae0ae9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "import pandas as pd\n",
    "\n",
    "def train_bpe(input_file, tokenizer_name, vocab_size=8000):\n",
    "    tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    trainer = BpeTrainer(vocab_size=vocab_size, special_tokens=[\"<pad>\", \"<s>\", \"</s>\", \"<unk>\"])\n",
    "    tokenizer.train([input_file], trainer)\n",
    "    tokenizer.save(f\"{tokenizer_name}.json\")\n",
    "\n",
    "train_bpe(lao_data_link, \"lo_tokenizer\")\n",
    "train_bpe(vi_data_link, \"vi_tokenizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cfdb3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  vi  \\\n",
      "0  Phán quyết của Tòa án quốc tế (PCA) năm 2016 l...   \n",
      "1  8 di sản văn hóa có hình dạng: Nhóm Di tích Tù...   \n",
      "2                                000 bà mẹ còn sống.   \n",
      "3  Việt Nam tổ chức thành công Diễn đàn Kinh tế T...   \n",
      "4  Một lần nữa, tại cuộc bầu cử lần này, số lượng...   \n",
      "\n",
      "                                                  lo  \\\n",
      "0  ຄຳຕັດສິນຂອງສານກຳມະການສາກົນ (PCA) ປີ 2016 ແມ່ນສ...   \n",
      "1  8 ມໍ​ລະ​ດົກ​​ວັດ​ທະ​ນະ​ທຳ​ມີ​ຮູບ​ຮ່າງຄື : ກຸ່ມ...   \n",
      "2                           000 ແມ່ທີ່ຍັງມີຊີວິດຢູ່.   \n",
      "3  ຫວຽດນາມ ຈັດຕັ້ງເວທີປາໄສເສດຖະກິດໂລກກ່ຽວກັບອາຊຽນ...   \n",
      "4  ອັນໜຶ່ງອີກ, ທີ່ການເລືອກຕັ້ງຄັ້ງນີ້, ຈຳນວນພັກກາ...   \n",
      "\n",
      "                                              lo_ids  \\\n",
      "0  [1, 4047, 6857, 1181, 770, 10, 4968, 11, 212, ...   \n",
      "1  [1, 26, 1344, 187, 202, 187, 2324, 285, 512, 1...   \n",
      "2         [1, 395, 1086, 2992, 234, 924, 272, 16, 2]   \n",
      "3  [1, 256, 1030, 6033, 713, 212, 561, 3799, 6033...   \n",
      "4  [1, 554, 837, 877, 14, 238, 2196, 2366, 14, 47...   \n",
      "\n",
      "                                              vi_ids  \n",
      "0  [1, 4201, 537, 190, 1836, 330, 266, 248, 9, 30...  \n",
      "1  [1, 25, 415, 374, 695, 459, 211, 615, 1689, 27...  \n",
      "2              [1, 310, 852, 2071, 520, 1057, 15, 2]  \n",
      "3  [1, 199, 196, 498, 423, 279, 290, 1070, 965, 9...  \n",
      "4  [1, 1605, 383, 1252, 13, 253, 424, 638, 413, 3...  \n"
     ]
    }
   ],
   "source": [
    "with open(vi_data_link, 'r', encoding='utf-8') as f_vi:\n",
    "    vi_sentences = f_vi.readlines()\n",
    "\n",
    "with open(lao_data_link, 'r', encoding='utf-8') as f_lo:\n",
    "    lo_sentences = f_lo.readlines()\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'vi': [sentence.strip() for sentence in vi_sentences],\n",
    "    'lo': [sentence.strip() for sentence in lo_sentences]\n",
    "})\n",
    "\n",
    "lo_tok = Tokenizer.from_file(\"lo_tokenizer.json\")\n",
    "vi_tok = Tokenizer.from_file(\"vi_tokenizer.json\")\n",
    "\n",
    "def encode_sentence(tok, sentence, add_special_tokens=True):\n",
    "    if add_special_tokens:\n",
    "        return [tok.token_to_id(\"<s>\")] + tok.encode(sentence).ids + [tok.token_to_id(\"</s>\")]\n",
    "    return tok.encode(sentence).ids\n",
    "\n",
    "df[\"lo_ids\"] = df[\"lo\"].apply(lambda x: encode_sentence(lo_tok, x))\n",
    "df[\"vi_ids\"] = df[\"vi\"].apply(lambda x: encode_sentence(vi_tok, x))\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04c10a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_ids, tgt_ids):\n",
    "        self.src = src_ids\n",
    "        self.tgt = tgt_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.src[idx]), torch.tensor(self.tgt[idx])\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = zip(*batch)\n",
    "    src_pad_id = vi_tok.token_to_id(\"<pad>\")\n",
    "    tgt_pad_id = lo_tok.token_to_id(\"<pad>\")\n",
    "    src_batch = pad_sequence(src_batch, batch_first=True, padding_value=src_pad_id)\n",
    "    tgt_batch = pad_sequence(tgt_batch, batch_first=True, padding_value=tgt_pad_id)\n",
    "    return src_batch, tgt_batch\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "src_train, src_valid, tgt_train, tgt_valid = train_test_split(\n",
    "    df[\"vi_ids\"].tolist(), df[\"lo_ids\"].tolist(), test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = TranslationDataset(src_train, tgt_train)\n",
    "valid_dataset = TranslationDataset(src_valid, tgt_valid)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78f7af7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vi_vocab_size = vi_tok.get_vocab_size()\n",
    "lo_vocab_size = lo_tok.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abd3da2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = vi_vocab_size\n",
    "OUTPUT_DIM = lo_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07373b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 57.13M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\dfine\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Epoch 1/100:   0%|          | 0/689 [00:00<?, ?it/s]c:\\Users\\Admin\\Documents\\GitHub\\Viet-Laos-Translation\\llama3_transformer_block.py:392: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  output = nn.functional.scaled_dot_product_attention(\n",
      "Epoch 1/100:   6%|▋         | 44/689 [00:07<01:45,  6.09it/s, loss=5.72]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m output \u001b[38;5;241m=\u001b[39m model(src_batch, tgt_batch[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     27\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, output\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), tgt_batch[:, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m---> 28\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     31\u001b[0m step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\dfine\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\dfine\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\dfine\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "encoder = Encoder(INPUT_DIM)\n",
    "decoder = Decoder(OUTPUT_DIM)\n",
    "model = TranslationModel(encoder, decoder)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Number of parameters: {num_params / 1e6:.2f}M\")\n",
    "\n",
    "# Optimizer và loss\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=lo_tok.token_to_id(\"<pad>\"))\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "num_epochs = 100\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for src_batch, tgt_batch in loop:\n",
    "        src_batch, tgt_batch = src_batch.to(device), tgt_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src_batch, tgt_batch[:, :-1])  # output shape: (B, T, vocab)\n",
    "        loss = criterion(output.reshape(-1, output.size(-1)), tgt_batch[:, 1:].reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for src_batch, tgt_batch in valid_loader:\n",
    "            src_batch, tgt_batch = src_batch.to(device), tgt_batch.to(device)\n",
    "            output = model(src_batch, tgt_batch[:, :-1])\n",
    "            loss = criterion(output.reshape(-1, output.size(-1)), tgt_batch[:, 1:].reshape(-1))\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(valid_loader)\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Scheduler update\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "        print(\"Saved best model.\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"last_model.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1dad9029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(model, src_sentence, src_tokenizer, tgt_tokenizer, device, max_len=50):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Encode source sentence\n",
    "        src_ids = [src_tokenizer.token_to_id(\"<s>\")] + src_tokenizer.encode(src_sentence).ids + [src_tokenizer.token_to_id(\"</s>\")]\n",
    "        src_tensor = torch.tensor(src_ids).unsqueeze(0).to(device)  # shape: (1, src_len)\n",
    "\n",
    "        # Forward pass through the encoder\n",
    "        encoder_out = model.encoder(src_tensor)\n",
    "\n",
    "        # Initialize target sequence with <s> token\n",
    "        tgt_ids = [tgt_tokenizer.token_to_id(\"<s>\")]\n",
    "        tgt_tensor = torch.tensor(tgt_ids).unsqueeze(0).to(device)  # shape: (1, 1)\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            # Forward pass through the decoder\n",
    "            output = model.decoder(tgt_tensor, encoder_out)  # shape: (1, len_so_far, vocab_size)\n",
    "            \n",
    "            # Get logits for the last token in the sequence\n",
    "            next_token_logits = output[0, -1]  # shape: (vocab_size,)\n",
    "            \n",
    "            # Get the token with the highest probability\n",
    "            next_token_id = torch.argmax(next_token_logits).item()\n",
    "            tgt_ids.append(next_token_id)\n",
    "\n",
    "            # If the </s> token is generated, stop decoding\n",
    "            if next_token_id == tgt_tokenizer.token_to_id(\"</s>\"):\n",
    "                break\n",
    "\n",
    "            # Update the target tensor with the newly generated token\n",
    "            tgt_tensor = torch.tensor(tgt_ids).unsqueeze(0).to(device)\n",
    "\n",
    "        # Decode the target sequence (excluding <s> and </s> tokens)\n",
    "        decoded_tokens = [tgt_tokenizer.id_to_token(tid) for tid in tgt_ids[1:-1]]\n",
    "        return \" \".join(decoded_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "106d577e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_model(model, data_loader, src_tokenizer, tgt_tokenizer, device, max_length=50):\n",
    "    \"\"\"\n",
    "    Evaluate the translation model using BLEU, METEOR, ROUGE-L, and BERTScore metrics.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained translation model\n",
    "        data_loader: DataLoader for evaluation dataset\n",
    "        src_tokenizer: Tokenizer for source language\n",
    "        tgt_tokenizer: Tokenizer for target language\n",
    "        device: Device to run the model on (cuda/cpu)\n",
    "        max_length: Maximum length for generated sequences\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing evaluation metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "    \n",
    "    for src_batch, tgt_batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "        for src_ids, tgt_ids in zip(src_batch, tgt_batch):\n",
    "            # Decode source sentence for translation\n",
    "            src_tokens = [src_tokenizer.id_to_token(sid.item()) for sid in src_ids \n",
    "                         if sid.item() not in [src_tokenizer.token_to_id(\"<s>\"), \n",
    "                                              src_tokenizer.token_to_id(\"</s>\"),\n",
    "                                              src_tokenizer.token_to_id(\"<pad>\")]]\n",
    "            src_sentence = \" \".join(src_tokens)\n",
    "            \n",
    "            # Generate translation\n",
    "            translated = translate(model, src_sentence, src_tokenizer, tgt_tokenizer, \n",
    "                                 device, max_length)\n",
    "            hypotheses.append(translated)\n",
    "            \n",
    "            # Decode reference\n",
    "            ref_tokens = [tgt_tokenizer.id_to_token(tid.item()) for tid in tgt_ids \n",
    "                         if tid.item() not in [tgt_tokenizer.token_to_id(\"<s>\"), \n",
    "                                              tgt_tokenizer.token_to_id(\"</s>\"),\n",
    "                                              tgt_tokenizer.token_to_id(\"<pad>\")]]\n",
    "            references.append([\" \".join(ref_tokens)])\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {}\n",
    "    \n",
    "    # BLEU score\n",
    "    bleu_score = corpus_bleu(references, hypotheses)\n",
    "    metrics['BLEU'] = bleu_score\n",
    "    \n",
    "    # METEOR score\n",
    "    meteor_scores = [meteor_score(ref[0].split(), hyp.split()) \n",
    "                    for ref, hyp in zip(references, hypotheses)]\n",
    "    metrics['METEOR'] = sum(meteor_scores) / len(meteor_scores)\n",
    "    \n",
    "    # ROUGE-L score\n",
    "    rouge_scorer_obj = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    rouge_scores = [rouge_scorer_obj.score(ref[0], hyp)['rougeL'].fmeasure \n",
    "                    for ref, hyp in zip(references, hypotheses)]\n",
    "    metrics['ROUGE-L'] = sum(rouge_scores) / len(rouge_scores)\n",
    "    \n",
    "    # BERTScore\n",
    "    P, R, F1 = bert_score(hypotheses, [ref[0] for ref in references], \n",
    "                         lang=\"en\", verbose=False)\n",
    "    metrics['BERTScore_F1'] = F1.mean().item()\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def print_metrics(metrics):\n",
    "    \"\"\"Print evaluation metrics in a formatted way.\"\"\"\n",
    "    print(\"\\nEvaluation Metrics:\")\n",
    "    print(\"-\" * 30)\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec64f3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   6%|▋         | 5/77 [00:26<06:13,  5.19s/it]"
     ]
    }
   ],
   "source": [
    "metrics = evaluate_model(model, valid_loader, vi_tok, lo_tok, device)\n",
    "print_metrics(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55553e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "def translate_beam_search(model, src_sentence, src_tokenizer, tgt_tokenizer, device, beam_width=3, max_len=50):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        src_ids = [src_tokenizer.token_to_id(\"<s>\")] + src_tokenizer.encode(src_sentence).ids + [src_tokenizer.token_to_id(\"</s>\")]\n",
    "        src_tensor = torch.tensor(src_ids).unsqueeze(0).to(device)\n",
    "        encoder_out = model.encoder(src_tensor)\n",
    "\n",
    "        beams = [(0, [tgt_tokenizer.token_to_id(\"<s>\")])]  # (score, token_ids)\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            new_beams = []\n",
    "            for score, seq in beams:\n",
    "                tgt_tensor = torch.tensor(seq).unsqueeze(0).to(device)\n",
    "                output = model.decoder(tgt_tensor, encoder_out)\n",
    "                next_logits = output[0, -1]\n",
    "                topk = torch.topk(next_logits, beam_width)\n",
    "\n",
    "                for i in range(beam_width):\n",
    "                    token_id = topk.indices[i].item()\n",
    "                    token_score = topk.values[i].item()\n",
    "                    new_seq = seq + [token_id]\n",
    "                    new_score = score + token_score\n",
    "                    new_beams.append((new_score, new_seq))\n",
    "\n",
    "            beams = heapq.nlargest(beam_width, new_beams, key=lambda x: x[0])\n",
    "\n",
    "            if all(seq[-1] == tgt_tokenizer.token_to_id(\"</s>\") for _, seq in beams):\n",
    "                break\n",
    "        best_seq = max(beams, key=lambda x: x[0])[1]\n",
    "        decoded = [tgt_tokenizer.id_to_token(i) for i in best_seq[1:-1]]\n",
    "        return \" \".join(decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a9e1ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated: 000 ຄົນ , ທ່ານ ​ ໃນ ​ ການ ​ ຄ້າ ​ ປະ ​ ເທດ ​ ໃນ ​ ການ ​ ຄ້າ ​ ໃນ ​ ການ ​ ຄ້າ ​ ການ ​ ຄ້າ ​ ສົ່ງ ​ ອອກ ​ ໃນ ​ ການ ​ ຄ້າ ​ ຮ່ວມ ​ ການ ​ ຄ້າ ​ ໃນ ​\n"
     ]
    }
   ],
   "source": [
    "sentence = \"di sản văn hóa\"\n",
    "translated = translate(model, sentence, vi_tok, lo_tok, device)\n",
    "print(\"Translated:\", translated)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dfine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
